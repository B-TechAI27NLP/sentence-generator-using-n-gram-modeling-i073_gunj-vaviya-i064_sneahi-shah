{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### NLP- EXPERIMENT 4\n",
        "name: Gunj Vaviya\n",
        "\n",
        "Roll No.: I073\n",
        "\n",
        "Batch: B.Tech A.I.\n",
        "\n",
        "Date: 26.08.25"
      ],
      "metadata": {
        "id": "-ffYZVwQGuCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "3eVM1ex-G3di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('gutenberg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THNxDJ8oG8hy",
        "outputId": "59d535a5-befa-47e4-ed62-34af867dd5b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hamlet_text = gutenberg.raw('shakespeare-hamlet.txt')"
      ],
      "metadata": {
        "id": "dCGqeuRIHUeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hamlet_tokens = gutenberg.words('shakespeare-hamlet.txt')"
      ],
      "metadata": {
        "id": "RbAoMKdjHWwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_lower = [token.lower() for token in hamlet_tokens]"
      ],
      "metadata": {
        "id": "XSLANeVvIdWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First 20 tokens of Hamlet:\", hamlet_tokens[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYyqd5kLHpvt",
        "outputId": "38e8ce9c-eba5-42ca-e8aa-3785cacedcd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 tokens of Hamlet: ['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', 'William', 'Shakespeare', '1599', ']', 'Actus', 'Primus', '.', 'Scoena', 'Prima', '.', 'Enter', 'Barnardo', 'and', 'Francisco']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TASK 1:"
      ],
      "metadata": {
        "id": "YC5Mwdo9KGCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " a.\tWrite a python program to find all unigrams, bigrams and trigrams present in the given corpus  (Student may use any corpus specified in Lab 3)."
      ],
      "metadata": {
        "id": "TB5rPsHWNRX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#UNIGRAMS:\n",
        "hamlet_unigrams = list(ngrams(hamlet_tokens, 1))\n",
        "print(\"\\nFirst 10 unigrams of Hamlet:\")\n",
        "print(hamlet_unigrams[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvfkPeD3H2Zq",
        "outputId": "15e2ddf7-5b07-43c9-d9d2-38f8e2919c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First 10 unigrams of Hamlet:\n",
            "[('[',), ('The',), ('Tragedie',), ('of',), ('Hamlet',), ('by',), ('William',), ('Shakespeare',), ('1599',), (']',)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BIGRAMS:\n",
        "hamlet_bigrams = list(ngrams(hamlet_tokens, 2))\n",
        "print(\"\\nFirst 10 bigrams of Hamlet:\")\n",
        "print(hamlet_bigrams[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5nNS90gHr16",
        "outputId": "c1e10b47-3a38-4bab-c10f-784ab7d67bf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First 10 bigrams of Hamlet:\n",
            "[('[', 'The'), ('The', 'Tragedie'), ('Tragedie', 'of'), ('of', 'Hamlet'), ('Hamlet', 'by'), ('by', 'William'), ('William', 'Shakespeare'), ('Shakespeare', '1599'), ('1599', ']'), (']', 'Actus')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TRIGRAMS:\n",
        "hamlet_trigrams = list(ngrams(hamlet_tokens, 3))\n",
        "print(\"\\nFirst 10 trigrams of Hamlet:\")\n",
        "print(hamlet_trigrams[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQM7DoN6IW0T",
        "outputId": "73ce062b-f839-4ff6-fd2a-54a477ed9b73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First 10 trigrams of Hamlet:\n",
            "[('[', 'The', 'Tragedie'), ('The', 'Tragedie', 'of'), ('Tragedie', 'of', 'Hamlet'), ('of', 'Hamlet', 'by'), ('Hamlet', 'by', 'William'), ('by', 'William', 'Shakespeare'), ('William', 'Shakespeare', '1599'), ('Shakespeare', '1599', ']'), ('1599', ']', 'Actus'), (']', 'Actus', 'Primus')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "c.\tWrite a python program to predict the next word based on bigram model and trigram model"
      ],
      "metadata": {
        "id": "8Vo2UfUjKJHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(ngrams_list):\n",
        "  model = {}\n",
        "  for gram in ngrams_list:\n",
        "    context = gram[:-1]\n",
        "    next_word = gram[-1]\n",
        "    if context not in model:\n",
        "      model[context] = Counter()\n",
        "      # Increment the count for the next word in the given context\n",
        "      model[context][next_word] += 1\n",
        "  return model"
      ],
      "metadata": {
        "id": "jfAtW8LOJEQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(model, context_tuple):\n",
        "    if context_tuple in model:\n",
        "        # Return the most common word in the Counter for this context\n",
        "        return model[context_tuple].most_common(1)[0][0]\n",
        "    else:\n",
        "        return \"No prediction available for this context.\""
      ],
      "metadata": {
        "id": "2Q2OvVWcK-21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the bigram model\n",
        "bigram_model = build_model(hamlet_bigrams)"
      ],
      "metadata": {
        "id": "vGyzVT1-LDvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrate bigram prediction\n",
        "print(\"\\n--- Bigram Model Prediction ---\")\n",
        "bigram_context = ('of',)\n",
        "predicted_word_bigram = predict_next_word(bigram_model, bigram_context)\n",
        "print(f\"Context: {bigram_context}\")\n",
        "print(f\"Predicted next word: '{predicted_word_bigram}'\")\n",
        "print(f\"(Based on the most common word following '{bigram_context[0]}')\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKZ0vWU0LJ3H",
        "outputId": "945f5d95-e95b-4d91-999b-7d2ea4978f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Bigram Model Prediction ---\n",
            "Context: ('of',)\n",
            "Predicted next word: 'Hamlet'\n",
            "(Based on the most common word following 'of')\n",
            "\n",
            "--- Bigram Model Prediction ---\n",
            "Context: ('of',)\n",
            "Predicted next word: 'Hamlet'\n",
            "(Based on the most common word following 'of')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b.\tWrite a python program to find the probability of the given statement using bigrams\n",
        "\n",
        "“This is my cat”\n",
        " by taking the following training data into consideration.\n",
        "'This is a dog’,\n",
        "'This is a cat',\n",
        " 'I love my cat',\n",
        " 'This is my name’\n"
      ],
      "metadata": {
        "id": "SbmeMHLyNdDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import bigrams\n",
        "from nltk.probability import ConditionalFreqDist, ConditionalProbDist, MLEProbDist"
      ],
      "metadata": {
        "id": "iIT0ZgG3NglE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wk_i5POYOEdb",
        "outputId": "c810bf2b-939c-4602-f1fc-c6577a6d318b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training sentences\n",
        "sentences = [\n",
        "    \"This is a dog\",\n",
        "    \"This is a cat\",\n",
        "    \"I love my cat\",\n",
        "    \"This is my name\"\n",
        "]"
      ],
      "metadata": {
        "id": "ZjgmcvDBOGNS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"This is my cat\""
      ],
      "metadata": {
        "id": "G9WuHI5rOKth"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70716443",
        "outputId": "2465a0f2-5d02-4560-904a-bca1e3b30d86"
      },
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(sentence):\n",
        "    tokens = nltk.word_tokenize(sentence.lower())\n",
        "    return [\"<s>\"] + tokens + [\"</s>\"]"
      ],
      "metadata": {
        "id": "HDryY-4qOMTR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize training sentences and generate bigrams\n",
        "train_tokens = [preprocess(sent) for sent in sentences]"
      ],
      "metadata": {
        "id": "9dudxn_cOO_B"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the list of tokens for unigram counts\n",
        "all_words = [word for sent in train_tokens for word in sent]"
      ],
      "metadata": {
        "id": "CgXv0F_XORMB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Conditional Frequency Distribution for bigrams\n",
        "cfd = ConditionalFreqDist()\n",
        "\n",
        "for sent in train_tokens:\n",
        "    for w1, w2 in bigrams(sent):\n",
        "        cfd[w1][w2] += 1"
      ],
      "metadata": {
        "id": "QZBgOPdvOZRB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Conditional Probability Distribution using MLE\n",
        "cpd = ConditionalProbDist(cfd, MLEProbDist)"
      ],
      "metadata": {
        "id": "RXcQAwCTOdeP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the test sentence\n",
        "test_tokens = preprocess(test_sentence)"
      ],
      "metadata": {
        "id": "WISGIxx2OfCH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate bigram probability of the test sentence\n",
        "prob = 1.0\n",
        "for w1, w2 in bigrams(test_tokens):\n",
        "    if w1 in cpd:\n",
        "        prob_w2_given_w1 = cpd[w1].prob(w2)\n",
        "        # If bigram probability is zero (not seen), overall prob is zero\n",
        "        if prob_w2_given_w1 == 0:\n",
        "            prob = 0\n",
        "            break\n",
        "        prob *= prob_w2_given_w1\n",
        "    else:\n",
        "        prob = 0\n",
        "        break\n",
        "\n",
        "print(f\"Probability of the sentence '{test_sentence}' is: {prob}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl4f7CHROh8m",
        "outputId": "95b524c7-1b1b-4c07-8068-a01a90a9ebee"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability of the sentence 'This is my cat' is: 0.125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "d.\tWrite a python program to calculate the perplexity of a test text based on n-gram probabilities"
      ],
      "metadata": {
        "id": "0eytN07QNgJ4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b631c1f"
      },
      "source": [
        "def preprocess(sentence):\n",
        "    tokens = nltk.word_tokenize(sentence.lower())\n",
        "    return [\"<s>\"] + tokens + [\"</s>\"]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6354d1ad"
      },
      "source": [
        "import math\n",
        "from nltk.util import bigrams\n",
        "\n",
        "def calculate_perplexity(sentence, cpd_model):\n",
        "    \"\"\"\n",
        "    Calculates the perplexity of a sentence given a bigram probability model.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The input sentence.\n",
        "        cpd_model (ConditionalProbDist): The bigram probability model (ConditionalProbDist).\n",
        "\n",
        "    Returns:\n",
        "        float: The perplexity of the sentence.\n",
        "    \"\"\"\n",
        "    # Preprocess the sentence (add <s> and </s>)\n",
        "    tokens = preprocess(sentence)\n",
        "    n = len(tokens) - 1 # Number of bigrams\n",
        "\n",
        "    # Calculate the probability of the sentence using the bigram model\n",
        "    sentence_prob = 1.0\n",
        "    for w1, w2 in bigrams(tokens):\n",
        "        if w1 in cpd_model and cpd_model[w1].prob(w2) > 0:\n",
        "            sentence_prob *= cpd_model[w1].prob(w2)\n",
        "        else:\n",
        "            # Handle unseen bigrams (assign a very small probability or use smoothing)\n",
        "            # For simplicity here, we'll assign a small probability\n",
        "            sentence_prob *= 1e-10 # Assign a small probability for unseen bigrams\n",
        "\n",
        "    # Calculate perplexity\n",
        "    if sentence_prob == 0:\n",
        "        return float('inf')  # Perplexity is infinite if probability is zero\n",
        "    perplexity = (1/sentence_prob)**(1/n)\n",
        "    return perplexity"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and print the perplexity of the test sentence\n",
        "perplexity_score = calculate_perplexity(test_sentence, cpd)\n",
        "print(f\"Perplexity of the sentence is: {perplexity_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-3NLm-xg_lW",
        "outputId": "f0f0472d-5ae3-4572-b929-92a96da0a8c4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of the sentence is: 1.5157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "######Conclusion:\n",
        "These three lines can be thought of as gathering the essential components for your task. import nltk brings in a massive toolkit designed for working with human language, giving you access to pre-built functions for tasks like tokenizing a sentence into words. From that toolkit, from nltk.util import ngrams selects a specific, powerful tool that is designed to group those words into sequences, like the two-word pairs needed for a bigram model. Finally, from collections import Counter provides a highly efficient tool that acts as an automatic tally sheet, quickly counting how many times each word or word pair appears in your training data, which is a fundamental step in calculating probabilities."
      ],
      "metadata": {
        "id": "qN1bODFig4uO"
      }
    }
  ]
}